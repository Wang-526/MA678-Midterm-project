---
title: "MA678 Midterm project"
author: "Yuxi Wang"
date: "2020/11/09"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(
'tidyverse',
'lme4',
'factoextra'
)
```

## Abstract



## 1. Introduction
# In this project, Santander invites people to help them identify which customers will make a specific transaction in the future. The data provided for this competition has the same structure as the real data they have available to solve this problem. The data is anonymized.
1Â·2
# Loading the data
```{r}
train <- read.csv(file = "/Users/mac/Desktop/midterm/train.csv")

# test <- read.csv(file = "/Users/mac/Desktop/midterm/test.csv")
```

## 2.Exploratory Data Analysis 

# 2.1 Data Summary

```{r}
head(train)
head(test)
```
After watching the first ten rows of two datasets, we can conclude:
1. As the project had presay, the names of every columns are anonymized, so we do not know what these variables' meaning.
2. There are many zero values present in the data.

```{r}
sum(is.na(train))
sum(is.na(test))
```
3. Neither the test set nor the training set has NAs.

However, since the data size is a little huge, we cannot easily use summary() or str() to have a simple understing of the data-set. So, I just count the rows and columns of training set and test set.
```{r}
print (paste("The number of records in training set:", dim(train)[1]))
print (paste("The number of predictors in training set:", dim(train)[2]))
print (paste("The number of records in test set:", dim(test)[1]))
print (paste("The number of predictors in test set:", dim(test)[2]))
```
As we can see, features in test set and training set are different. But this is because the tset set does not have target, and that is what I will do in test set if I take part in the competition. So, it does not metter. Also, test set is almost 10 times as that of training set. So, in order to easily running algorithms, I may mainly use training set to modeling.

# 2.2 Target variable
Since the purpose of training set is to let us use other predictors to fit the target variable. 
It is necessary to know as much information about the target variable as possible.

As we can see, the dataset is very massive, so I have to rank the value to see the distribution plot of it.
```{r}
ggplot(train,aes(x=target,alpha=1/10,fill='red'))+
  geom_density()+
  guides(fill=FALSE,alpha=FALSE)
  
```

If we analysis the plot we can find that the distribution with majority of the data points having low value, and a huge amount of the data is 0. 
```{r}
ggplot(train,aes(x=log(target),y=..density..))+
  geom_histogram(fill='cornsilk',color='grey60',size=.2, alpha=.5)+
  geom_density()
```
So, now we know the desity of the target variable. Also, we know that if we use target as dependent variable to fit the model, it is better to use log(target) instead of target.


# 2.3 Other predictors
There are near 5000 columns in the training set, which means I need to do the feature selection, in order to better fit a model, because rows are even less than columns.
Since there are so many predictors, and by observing the data structure output by head(train), we can predict that there will be a lot of 0s in it, so we first remove the predictors and then, we can calculate the condition number to see if there are multicollinearity.

```{r}
# Because predictors with all 0s are meaningless and will affect our matrix operations, we remove them.
# delete the 0 variable
col_sub = apply(train, 2, function(col) any (col !=0 ))
# Subset as usual
new_train <- train[,col_sub]

print (paste("The number of predictors that are not all 0s are:", dim(new_train)[2]))

```
So we know that there are 256 variables that all equals 0. Also, since they are all 0, they are meaningless in the model.


However, there are still too many predictros.
```{r}
XX <- cor(new_train[,3:4737]) #Calculate the correlation coefficient matrix between independent variables
kappa(XX,exact=TRUE) #calculate the condition number accurately
# eigen(XX) 
# The output is too huge, and I use another similar methods in modeling part, so I won't run this step again.
```
Since $6.187769e+19 >> 1000$, it shows that there is serious multicollinearity between the independent variables.

We can use Characteristic root judgment or Variance Inflation Factor method and also step regression may can deal with this method. But they are both too slow to use step by step. At the mean time, the result  I get may not meet my ideas. So, I add a PCA model in the modeling part in order to deal with the multicollinearity, and also Simultaneously reduce the dimension of the independent variables.


## 3. Modeling

# 3.1 PCA
Principal Component Analysis is a statistical method that transforms a group of potentially correlated variables into a group of linearly uncorrelated variables through orthogonal transformation.

Since the predictor I used is encrypted, that is, I don't know its specific meaning, so I don't need to estimate the actual meaning of each new variable temporarily by using the principal component. However I acknowledge that in the specific analysis process, it is very necessary to understand the actual meaning of each variable as much as possible.

Now, I am going to do the PCA. Here, since there are more features than observation, the function I use is "prcomp". Although it has the same purpose as the commonly used "princomp", there are some differences. "prcomp" is done by singular value decomposition, whereas "princomp" is done by spectral decomposition on the correlation or covariance matrix, and the former is preferred for numerical accuracy.
```{r}
sample <- new_train[,3:4737]
sample.pr <- prcomp(sample,scale=TRUE)
pre <- predict(sample.pr)
# head(sample.pr)
# summary(sample.pr)
screeplot(sample.pr[1:10],type='lines') 
data <- data.frame(x1=pre[,1],x2=pre[,2],x3=pre[,3],x4=pre[,4],x5=pre[,5],x6=pre[,6],x7=pre[,7],x8=pre[,8],x9=pre[,9],x10=pre[,10],y=new_train$target)
```
Also, the outcome is too huge, so I just show the scree plot of the PCA. 

At first glance, the results shown in this plot are not good. But this is mainly caused by the existence of a large number of variables with only one or a little values, and they are independent. After I understand the variable structure of our original data. When the principal component is 5, a obvious inflection point appears. After the five variables produced by the principal component can cover 60% of the original variable, the contribution rate of the principal component sharply decreased. So we can say that there are independent variables with a large proportion but only a small number of values.

Therefore, the method I use is to use the top ten principal components, although they can only represent 80% of the original independent variables. As for the other 20%, without greatly expanding the number of principal components, it is difficult for me to use them without considering the specific application background because they are not universal behaviors to the bank's customers.


# 3.2 Fit the model
# 3.2.1 linear regression/ principal component regression
After performing principal component analysis, I first performed linear regression. Because it was done after principal component analysis, some individuals also called it principal component regression to gain a preliminary understanding of regression modeling for this data set.
Just a reminder, in the EDA part I analysis the density of target, andwe know that if we use target as dependent variable to fit the model, it is better to use log(target) instead of target.
```{r}
fit<-lm(log(y)~x1+x2+x3+x4+x5+x6+x7+x8+x9+x10, data=data)
summary(fit)
```

# 3.2.1 Multilevel regression
Since the result we want is to determine whether the customer has value, we can get a structured data type after converting the original variable. First, we cluster all the customers that are owned by the observations, that is, we can divide customers into several categories according to the behavior of the account. At the same time, we can classify their value levels according to the 1/3 and 2/3 quantiles. Then, according to the behavior of their account, first get the type of the customer, and then get the value of transactions for each potential customer.
```{r}
# Use tertiles to divide target into three parts, which are defined as low-value, medium-value, and high-value customers
limit1 <- quantile(data$y,c(0.33,0.67))[1]
limit2 <- quantile(data$y,c(0.33,0.67))[2]
# And I define by myself, customers whose target value is above the median are valuable.
for(i in 1:4459)
  if (data$y[i] > limit2) {
    data$valuable[i] <- 3
  }else if(data$y[i] > limit1&&data$y[i] < limit2){
    data$valuable[i] <- 2
  }else{
    data$valuable[i] <- 1
  }

# Cluster all the observations
fviz_nbclust(data, kmeans, method = "wss") + 
  geom_vline(xintercept = 3, linetype = 2)
km_result <- kmeans(data, 3, nstart = 24)

#Extract class labels and merge with the original data
data <- cbind(data, cluster = km_result$cluster)


```

```{r warning=FALSE, message=FALSE}
# 
model <- lmer(log(y) ~1 +x1+x2+x3+x4+x5+x6+x7+x8+x9+x10+ (1|cluster) + valuable,REML=FALSE, data=data)
summary(model)
confint(model)
```


## 4.Results 

```{r}

```


## 5.Discussion


















